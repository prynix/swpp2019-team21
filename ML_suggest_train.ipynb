{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swsnu/swpp2019-team21/blob/master/ML_suggest_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcSqFM7Kv_pa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install konlpy       # Python 3.x\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!pip3 install JPype1-py3\n",
        "\n",
        "import os\n",
        "os.chdir('/tmp/')\n",
        "!curl -LO https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.1.tar.gz\n",
        "!tar zxfv mecab-0.996-ko-0.9.1.tar.gz\n",
        "os.chdir('/tmp/mecab-0.996-ko-0.9.1')\n",
        "!./configure\n",
        "!make\n",
        "!make check\n",
        "!make install\n",
        "\n",
        "os.chdir('/tmp/')\n",
        "!wget -O m4-1.4.9.tar.gz http://ftp.gnu.org/gnu/m4/m4-1.4.9.tar.gz\n",
        "!tar -zvxf m4-1.4.9.tar.gz\n",
        "os.chdir('/tmp/m4-1.4.9')\n",
        "!./configure\n",
        "!make\n",
        "!make install\n",
        "\n",
        "os.chdir('/tmp')\n",
        "!curl -OL http://ftp.gnu.org/gnu/autoconf/autoconf-2.69.tar.gz\n",
        "!tar xzf autoconf-2.69.tar.gz\n",
        "os.chdir('/tmp/autoconf-2.69')\n",
        "!./configure --prefix=/usr/local\n",
        "!make\n",
        "!make install\n",
        "!export PATH=/usr/local/bin\n",
        "\n",
        "os.chdir('/tmp')\n",
        "!curl -LO http://ftp.gnu.org/gnu/automake/automake-1.11.tar.gz\n",
        "!tar -zxvf automake-1.11.tar.gz\n",
        "os.chdir('/tmp/automake-1.11')\n",
        "!./configure\n",
        "!make\n",
        "!make install\n",
        "\n",
        "os.chdir('/tmp')\n",
        "!curl -LO https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.0.1-20150920.tar.gz\n",
        "!tar -zxvf mecab-ko-dic-2.0.1-20150920.tar.gz\n",
        "os.chdir('/tmp/mecab-ko-dic-2.0.1-20150920')\n",
        "!ldconfig\n",
        "!ldconfig -p | grep /usr/local/lib\n",
        "!./autogen.sh\n",
        "!./configure\n",
        "!make\n",
        "!make install\n",
        "\n",
        "os.chdir('/content')\n",
        "\n",
        "!git clone https://bitbucket.org/eunjeon/mecab-python-0.996.git\n",
        "os.chdir('/content/mecab-python-0.996')\n",
        "\n",
        "!python3 setup.py build\n",
        "!python3 setup.py install\n",
        "\n",
        "from konlpy.tag import Mecab\n",
        "m = Mecab()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsjgzVpoyDVk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRotNIrYwBCr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np, pandas as pd\n",
        "import pickle, os, re, glob\n",
        "import gensim\n",
        "import glob\n",
        "import konlpy\n",
        "import nltk\n",
        "from konlpy.tag import Mecab\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tag import pos_tag\n",
        "import random, logging"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVpomylxxUQb",
        "colab_type": "code",
        "outputId": "fa4cd579-5383-46e1-e2cb-fc4e1eb80102",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "os.chdir('/content/drive/My Drive/SWPP_ML/')\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "\n",
        "tokenizer = Mecab()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rv6I_OvdxXHT",
        "colab_type": "code",
        "outputId": "4ee08370-d852-4fcd-fb41-404dccccb401",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = gensim.models.Word2Vec.load('ko.bin')\n",
        "\n",
        "data = glob.glob('./data/*')\n",
        "\n",
        "#############################################################################################################\n",
        "#############################################################################################################\n",
        "\n",
        "raw = {}\n",
        "raw['KOR'] = []\n",
        "raw['ENG'] = []\n",
        "\n",
        "for dat in data:\n",
        "    for col in ['KOR', 'ENG']:\n",
        "        d = pd.read_excel(dat)\n",
        "        print(dat, d.columns)\n",
        "        raw[col] += list(d[col])\n",
        "\n",
        "dat = pd.DataFrame(raw)\n",
        "\n",
        "print(\"Data Loaded\")\n",
        "print(\"#\"*100)\n",
        "#############################################################################################################\n",
        "#############################################################################################################\n",
        "\n",
        "dat.KOR = dat.KOR.apply(tokenizer.pos)\n",
        "dat.ENG = dat.ENG.apply(word_tokenize)\n",
        "dat.ENG = dat.ENG.apply(pos_tag)\n",
        "\n",
        "stops = set(stopwords.words('english'))\n",
        "\n",
        "print(\"Processing on Korean Data\")\n",
        "\n",
        "for i in range(len(dat)):\n",
        "    dat.KOR[i] = list(map(lambda x : x[0], list(filter(lambda x : x[1] in ['NNG', 'NNP'], dat.KOR[i]))))\n",
        "\n",
        "print(\"Processing on English Data\")\n",
        "\n",
        "for i in range(len(dat)):\n",
        "    dat.ENG[i] = list(map(lambda x : x[0], list(filter(lambda x : x[1] in ['NN', 'NNP'], dat.ENG[i]))))\n",
        "\n",
        "print(\"Data Preprocessed\")\n",
        "print(\"#\"*100)\n",
        "#############################################################################################################\n",
        "#############################################################################################################\n",
        "\n",
        "ipt = []\n",
        "\n",
        "print(\"Merging Sentences\")\n",
        "\n",
        "for i in range(len(dat)):\n",
        "    ipt.append(dat.KOR[i] + dat.ENG[i])\n",
        "\n",
        "for i in range(len(ipt)):\n",
        "    random.shuffle(ipt[i])\n",
        "\n",
        "print(\"Input Set\")\n",
        "print(\"#\"*100)\n",
        "#############################################################################################################\n",
        "#############################################################################################################\n",
        "\n",
        "model.build_vocab(sentences=ipt, update=True, min_count=20)\n",
        "model.train(sentences=ipt, epochs=10, total_examples=len(ipt))\n",
        "\n",
        "model.save('models_engadd.bin')\n",
        "\n",
        "print(\"Done!\")\n",
        "print(\"#\"*100)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-11-25 09:51:53,653 : INFO : loading Word2Vec object from ko.bin\n",
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
            "2019-11-25 09:51:53,831 : INFO : setting ignored attribute syn0norm to None\n",
            "2019-11-25 09:51:53,832 : INFO : setting ignored attribute cum_table to None\n",
            "2019-11-25 09:51:53,835 : INFO : Model saved using code from earlier Gensim Version. Re-loading old model in a compatible way.\n",
            "2019-11-25 09:51:53,837 : INFO : loading Word2Vec object from ko.bin\n",
            "2019-11-25 09:51:54,104 : INFO : setting ignored attribute syn0norm to None\n",
            "2019-11-25 09:51:54,105 : INFO : setting ignored attribute cum_table to None\n",
            "2019-11-25 09:51:54,108 : INFO : loaded ko.bin\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "./data/6.문어체-지자체웹사이트.xlsx Index(['ID', '제목', 'URL', 'KOR', 'NMT', 'ENG'], dtype='object')\n",
            "./data/6.문어체-지자체웹사이트.xlsx Index(['ID', '제목', 'URL', 'KOR', 'NMT', 'ENG'], dtype='object')\n",
            "./data/1.구어체.xlsx Index(['mid_sid', 'KOR', 'ENG'], dtype='object')\n",
            "./data/1.구어체.xlsx Index(['mid_sid', 'KOR', 'ENG'], dtype='object')\n",
            "./data/4.문어체-한국문화.xlsx Index(['ID', '분야', 'KOR', 'NMT', 'ENG'], dtype='object')\n",
            "./data/4.문어체-한국문화.xlsx Index(['ID', '분야', 'KOR', 'NMT', 'ENG'], dtype='object')\n",
            "./data/5.문어체-조례.xlsx Index(['ID', '분야', 'KOR', 'NMT', 'ENG'], dtype='object')\n",
            "./data/5.문어체-조례.xlsx Index(['ID', '분야', 'KOR', 'NMT', 'ENG'], dtype='object')\n",
            "./data/3.문어체-뉴스-2.xlsx Index(['id', '날짜', '자동분류_1', '자동분류_2', '자동분류_3', 'URL', '언론사', 'KOR', 'ENG'], dtype='object')\n",
            "./data/3.문어체-뉴스-2.xlsx Index(['id', '날짜', '자동분류_1', '자동분류_2', '자동분류_3', 'URL', '언론사', 'KOR', 'ENG'], dtype='object')\n",
            "./data/2.대화체.xlsx Index(['대분류', '소분류', '상황', 'Set Nr.', '발화자', 'KOR', 'ENG', '영어 검수'], dtype='object')\n",
            "./data/2.대화체.xlsx Index(['대분류', '소분류', '상황', 'Set Nr.', '발화자', 'KOR', 'ENG', '영어 검수'], dtype='object')\n",
            "./data/3.문어체-뉴스.xlsx Index(['id', '날짜', '자동분류_1', '자동분류_2', '자동분류_3', 'URL', '언론사', 'KOR', 'NMT 번역',\n",
            "       'ENG'],\n",
            "      dtype='object')\n",
            "./data/3.문어체-뉴스.xlsx Index(['id', '날짜', '자동분류_1', '자동분류_2', '자동분류_3', 'URL', '언론사', 'KOR', 'NMT 번역',\n",
            "       'ENG'],\n",
            "      dtype='object')\n",
            "Data Loaded\n",
            "####################################################################################################\n",
            "Processing on Korean Data\n",
            "Processing on English Data\n",
            "Data Preprocessed\n",
            "####################################################################################################\n",
            "Merging Sentences\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-11-25 10:07:18,876 : INFO : collecting all words and their counts\n",
            "2019-11-25 10:07:18,880 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2019-11-25 10:07:18,909 : INFO : PROGRESS: at sentence #10000, processed 61372 words, keeping 11428 word types\n",
            "2019-11-25 10:07:18,928 : INFO : PROGRESS: at sentence #20000, processed 104786 words, keeping 15337 word types\n",
            "2019-11-25 10:07:18,949 : INFO : PROGRESS: at sentence #30000, processed 149724 words, keeping 18801 word types\n",
            "2019-11-25 10:07:18,969 : INFO : PROGRESS: at sentence #40000, processed 193093 words, keeping 21254 word types\n",
            "2019-11-25 10:07:18,991 : INFO : PROGRESS: at sentence #50000, processed 240309 words, keeping 23793 word types\n",
            "2019-11-25 10:07:19,014 : INFO : PROGRESS: at sentence #60000, processed 292314 words, keeping 26939 word types\n",
            "2019-11-25 10:07:19,035 : INFO : PROGRESS: at sentence #70000, processed 340012 words, keeping 29034 word types\n",
            "2019-11-25 10:07:19,076 : INFO : PROGRESS: at sentence #80000, processed 446921 words, keeping 38930 word types\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Input Set\n",
            "####################################################################################################\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-11-25 10:07:19,153 : INFO : PROGRESS: at sentence #90000, processed 683484 words, keeping 43602 word types\n",
            "2019-11-25 10:07:19,227 : INFO : PROGRESS: at sentence #100000, processed 906322 words, keeping 45301 word types\n",
            "2019-11-25 10:07:19,317 : INFO : PROGRESS: at sentence #110000, processed 1161022 words, keeping 62400 word types\n",
            "2019-11-25 10:07:19,341 : INFO : PROGRESS: at sentence #120000, processed 1217547 words, keeping 63126 word types\n",
            "2019-11-25 10:07:19,366 : INFO : PROGRESS: at sentence #130000, processed 1273636 words, keeping 64003 word types\n",
            "2019-11-25 10:07:19,463 : INFO : PROGRESS: at sentence #140000, processed 1533840 words, keeping 75175 word types\n",
            "2019-11-25 10:07:19,562 : INFO : PROGRESS: at sentence #150000, processed 1809568 words, keeping 84980 word types\n",
            "2019-11-25 10:07:19,652 : INFO : PROGRESS: at sentence #160000, processed 2059052 words, keeping 93114 word types\n",
            "2019-11-25 10:07:19,739 : INFO : PROGRESS: at sentence #170000, processed 2305037 words, keeping 104104 word types\n",
            "2019-11-25 10:07:19,822 : INFO : PROGRESS: at sentence #180000, processed 2541720 words, keeping 108917 word types\n",
            "2019-11-25 10:07:19,858 : INFO : collected 110786 word types from a corpus of 2632247 raw words and 183780 sentences\n",
            "2019-11-25 10:07:19,859 : INFO : Updating model with new vocabulary\n",
            "2019-11-25 10:07:19,935 : INFO : New added 13346 unique words (10% of original 124132) and increased the count of 13346 pre-existing words (10% of original 124132)\n",
            "2019-11-25 10:07:20,021 : INFO : deleting the raw counts dictionary of 110786 items\n",
            "2019-11-25 10:07:20,026 : INFO : sample=0.001 downsamples 20 most-common words\n",
            "2019-11-25 10:07:20,026 : INFO : downsampling leaves estimated 4643080 word corpus (199.2% of prior 2330565)\n",
            "2019-11-25 10:07:20,091 : INFO : estimated required memory for 26692 words and 200 dimensions: 56053200 bytes\n",
            "2019-11-25 10:07:20,092 : INFO : updating layer weights\n",
            "2019-11-25 10:07:21,495 : INFO : training model with 3 workers on 36275 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
            "2019-11-25 10:07:22,528 : INFO : EPOCH 1 - PROGRESS: at 44.30% examples, 427621 words/s, in_qsize 5, out_qsize 0\n",
            "2019-11-25 10:07:23,541 : INFO : EPOCH 1 - PROGRESS: at 55.83% examples, 449862 words/s, in_qsize 5, out_qsize 0\n",
            "2019-11-25 10:07:24,558 : INFO : EPOCH 1 - PROGRESS: at 75.03% examples, 450479 words/s, in_qsize 5, out_qsize 0\n",
            "2019-11-25 10:07:25,572 : INFO : EPOCH 1 - PROGRESS: at 85.54% examples, 450831 words/s, in_qsize 5, out_qsize 0\n",
            "2019-11-25 10:07:26,593 : INFO : EPOCH 1 - PROGRESS: at 97.32% examples, 452528 words/s, in_qsize 5, out_qsize 0\n",
            "2019-11-25 10:07:26,793 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-11-25 10:07:26,798 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-11-25 10:07:26,809 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-11-25 10:07:26,810 : INFO : EPOCH - 1 : training on 2632247 raw words (2398768 effective words) took 5.3s, 453836 effective words/s\n",
            "2019-11-25 10:07:27,837 : INFO : EPOCH 2 - PROGRESS: at 44.78% examples, 442684 words/s, in_qsize 5, out_qsize 0\n",
            "2019-11-25 10:07:28,875 : INFO : EPOCH 2 - PROGRESS: at 56.86% examples, 463920 words/s, in_qsize 5, out_qsize 0\n",
            "2019-11-25 10:07:29,911 : INFO : EPOCH 2 - PROGRESS: at 76.48% examples, 462936 words/s, in_qsize 5, out_qsize 0\n",
            "2019-11-25 10:07:30,930 : INFO : EPOCH 2 - PROGRESS: at 86.83% examples, 457532 words/s, in_qsize 3, out_qsize 2\n",
            "2019-11-25 10:07:31,954 : INFO : EPOCH 2 - PROGRESS: at 98.75% examples, 457889 words/s, in_qsize 5, out_qsize 0\n",
            "2019-11-25 10:07:32,016 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-11-25 10:07:32,034 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-11-25 10:07:32,035 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-11-25 10:07:32,037 : INFO : EPOCH - 2 : training on 2632247 raw words (2398767 effective words) took 5.2s, 460163 effective words/s\n",
            "2019-11-25 10:07:33,049 : INFO : EPOCH 3 - PROGRESS: at 44.57% examples, 437881 words/s, in_qsize 5, out_qsize 0\n",
            "2019-11-25 10:07:34,067 : INFO : EPOCH 3 - PROGRESS: at 56.45% examples, 462263 words/s, in_qsize 5, out_qsize 0\n",
            "2019-11-25 10:07:35,079 : INFO : EPOCH 3 - PROGRESS: at 75.86% examples, 462336 words/s, in_qsize 5, out_qsize 0\n",
            "2019-11-25 10:07:36,097 : INFO : EPOCH 3 - PROGRESS: at 86.40% examples, 459556 words/s, in_qsize 5, out_qsize 0\n",
            "2019-11-25 10:07:37,110 : INFO : EPOCH 3 - PROGRESS: at 98.05% examples, 458476 words/s, in_qsize 5, out_qsize 0\n",
            "2019-11-25 10:07:37,233 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-11-25 10:07:37,251 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-11-25 10:07:37,261 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-11-25 10:07:37,262 : INFO : EPOCH - 3 : training on 2632247 raw words (2398662 effective words) took 5.2s, 460002 effective words/s\n",
            "2019-11-25 10:07:38,307 : INFO : EPOCH 4 - PROGRESS: at 44.78% examples, 433853 words/s, in_qsize 5, out_qsize 0\n",
            "2019-11-25 10:07:39,311 : INFO : EPOCH 4 - PROGRESS: at 56.86% examples, 466877 words/s, in_qsize 5, out_qsize 0\n",
            "2019-11-25 10:07:40,334 : INFO : EPOCH 4 - PROGRESS: at 76.27% examples, 463925 words/s, in_qsize 4, out_qsize 1\n",
            "2019-11-25 10:07:41,363 : INFO : EPOCH 4 - PROGRESS: at 87.48% examples, 465744 words/s, in_qsize 5, out_qsize 0\n",
            "2019-11-25 10:07:42,368 : INFO : EPOCH 4 - PROGRESS: at 99.20% examples, 464610 words/s, in_qsize 3, out_qsize 1\n",
            "2019-11-25 10:07:42,376 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-11-25 10:07:42,391 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-11-25 10:07:42,403 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-11-25 10:07:42,404 : INFO : EPOCH - 4 : training on 2632247 raw words (2398653 effective words) took 5.1s, 467558 effective words/s\n",
            "2019-11-25 10:07:43,420 : INFO : EPOCH 5 - PROGRESS: at 44.78% examples, 445997 words/s, in_qsize 5, out_qsize 0\n",
            "2019-11-25 10:07:44,426 : INFO : EPOCH 5 - PROGRESS: at 56.66% examples, 468730 words/s, in_qsize 5, out_qsize 1\n",
            "2019-11-25 10:07:45,463 : INFO : EPOCH 5 - PROGRESS: at 76.48% examples, 468659 words/s, in_qsize 5, out_qsize 0\n",
            "2019-11-25 10:07:46,463 : INFO : EPOCH 5 - PROGRESS: at 87.03% examples, 466337 words/s, in_qsize 5, out_qsize 0\n",
            "2019-11-25 10:07:47,473 : INFO : EPOCH 5 - PROGRESS: at 98.97% examples, 466073 words/s, in_qsize 4, out_qsize 1\n",
            "2019-11-25 10:07:47,508 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-11-25 10:07:47,523 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-11-25 10:07:47,526 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-11-25 10:07:47,528 : INFO : EPOCH - 5 : training on 2632247 raw words (2398656 effective words) took 5.1s, 469124 effective words/s\n",
            "2019-11-25 10:07:48,550 : INFO : EPOCH 6 - PROGRESS: at 45.00% examples, 453603 words/s, in_qsize 4, out_qsize 1\n",
            "2019-11-25 10:07:49,560 : INFO : EPOCH 6 - PROGRESS: at 57.08% examples, 475487 words/s, in_qsize 5, out_qsize 0\n",
            "2019-11-25 10:07:50,595 : INFO : EPOCH 6 - PROGRESS: at 77.10% examples, 476431 words/s, in_qsize 5, out_qsize 0\n",
            "2019-11-25 10:07:51,599 : INFO : EPOCH 6 - PROGRESS: at 87.93% examples, 473273 words/s, in_qsize 6, out_qsize 0\n",
            "2019-11-25 10:07:52,550 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-11-25 10:07:52,572 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-11-25 10:07:52,574 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-11-25 10:07:52,575 : INFO : EPOCH - 6 : training on 2632247 raw words (2398576 effective words) took 5.0s, 476394 effective words/s\n",
            "2019-11-25 10:07:53,616 : INFO : EPOCH 7 - PROGRESS: at 45.22% examples, 454276 words/s, in_qsize 5, out_qsize 0\n",
            "2019-11-25 10:07:54,622 : INFO : EPOCH 7 - PROGRESS: at 57.52% examples, 480428 words/s, in_qsize 5, out_qsize 0\n",
            "2019-11-25 10:07:55,656 : INFO : EPOCH 7 - PROGRESS: at 76.89% examples, 471311 words/s, in_qsize 5, out_qsize 2\n",
            "2019-11-25 10:07:56,673 : INFO : EPOCH 7 - PROGRESS: at 87.74% examples, 468233 words/s, in_qsize 4, out_qsize 1\n",
            "2019-11-25 10:07:57,662 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-11-25 10:07:57,675 : INFO : EPOCH 7 - PROGRESS: at 99.78% examples, 469643 words/s, in_qsize 1, out_qsize 1\n",
            "2019-11-25 10:07:57,677 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-11-25 10:07:57,679 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-11-25 10:07:57,680 : INFO : EPOCH - 7 : training on 2632247 raw words (2398699 effective words) took 5.1s, 470984 effective words/s\n",
            "2019-11-25 10:07:58,708 : INFO : EPOCH 8 - PROGRESS: at 45.20% examples, 460618 words/s, in_qsize 5, out_qsize 0\n",
            "2019-11-25 10:07:59,713 : INFO : EPOCH 8 - PROGRESS: at 57.30% examples, 479545 words/s, in_qsize 5, out_qsize 0\n",
            "2019-11-25 10:08:00,735 : INFO : EPOCH 8 - PROGRESS: at 77.32% examples, 481370 words/s, in_qsize 5, out_qsize 0\n",
            "2019-11-25 10:08:01,750 : INFO : EPOCH 8 - PROGRESS: at 88.17% examples, 475614 words/s, in_qsize 5, out_qsize 0\n",
            "2019-11-25 10:08:02,692 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-11-25 10:08:02,693 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-11-25 10:08:02,710 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-11-25 10:08:02,711 : INFO : EPOCH - 8 : training on 2632247 raw words (2398844 effective words) took 5.0s, 478001 effective words/s\n",
            "2019-11-25 10:08:03,759 : INFO : EPOCH 9 - PROGRESS: at 45.42% examples, 467187 words/s, in_qsize 5, out_qsize 0\n",
            "2019-11-25 10:08:04,769 : INFO : EPOCH 9 - PROGRESS: at 57.71% examples, 485971 words/s, in_qsize 4, out_qsize 1\n",
            "2019-11-25 10:08:05,777 : INFO : EPOCH 9 - PROGRESS: at 77.52% examples, 484829 words/s, in_qsize 5, out_qsize 0\n",
            "2019-11-25 10:08:06,809 : INFO : EPOCH 9 - PROGRESS: at 88.62% examples, 478212 words/s, in_qsize 5, out_qsize 1\n",
            "2019-11-25 10:08:07,709 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-11-25 10:08:07,723 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-11-25 10:08:07,725 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-11-25 10:08:07,727 : INFO : EPOCH - 9 : training on 2632247 raw words (2398786 effective words) took 5.0s, 480780 effective words/s\n",
            "2019-11-25 10:08:08,777 : INFO : EPOCH 10 - PROGRESS: at 45.42% examples, 460375 words/s, in_qsize 4, out_qsize 1\n",
            "2019-11-25 10:08:09,780 : INFO : EPOCH 10 - PROGRESS: at 57.72% examples, 483837 words/s, in_qsize 5, out_qsize 0\n",
            "2019-11-25 10:08:10,799 : INFO : EPOCH 10 - PROGRESS: at 77.32% examples, 478662 words/s, in_qsize 4, out_qsize 1\n",
            "2019-11-25 10:08:11,803 : INFO : EPOCH 10 - PROGRESS: at 88.63% examples, 479108 words/s, in_qsize 5, out_qsize 0\n",
            "2019-11-25 10:08:12,721 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-11-25 10:08:12,725 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-11-25 10:08:12,733 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-11-25 10:08:12,734 : INFO : EPOCH - 10 : training on 2632247 raw words (2398709 effective words) took 5.0s, 480389 effective words/s\n",
            "2019-11-25 10:08:12,735 : INFO : training on a 26322470 raw words (23987120 effective words) took 51.2s, 468142 effective words/s\n",
            "2019-11-25 10:08:12,737 : INFO : saving Word2Vec object under models_engadd.bin, separately None\n",
            "2019-11-25 10:08:12,738 : INFO : not storing attribute vectors_norm\n",
            "2019-11-25 10:08:12,740 : INFO : not storing attribute cum_table\n",
            "2019-11-25 10:08:13,478 : INFO : saved models_engadd.bin\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Done!\n",
            "####################################################################################################\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}